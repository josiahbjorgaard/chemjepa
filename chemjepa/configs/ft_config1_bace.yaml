#label_col: "Labels"
#estart: "" #'training_output_21_31_23_10_2023'
#wandb_name: "No Name"
#wandb_restart: ""
epochs: 1000
#start_epoch: 0
batch_size: 32 #384
num_warmup_steps: 300
#lr_scheduler_type: "cosine"
lr: 1e-5
#output_dir: "" #datetime.now().strftime('training_output_%H_%M_%d_%m_%Y')
#dataset: "/shared/encoded_pubchem10M" #"/shared/fcaa53cd-ba57-4bfe-af9c-eaa958f95c1a_combined_all"
#dataset: '/shared/tox21_hepg2_combined_split'
dataset: '/shared/BACE_dataset'
#dataset: '/shared/toxicity_split_cleaned'
smiles_col: "canonical_smiles"
split: 0.1
ds_frac: 1.0
ds_seed: 42
seed: 42
dropout: 0.1
#clip: 0.0
#n_step_checkpoint: 20000
#run_eval_loop: True
#embedding: CN(new_allowed=True)  # None #{}
num_mask: 0 #4
transform: "None" #"old"
mask: False
rotate: ""
run_predictor: True #False
project_name: ChemJEPAFT
decoder:
        #type: Attentive #MLP
  type: MLP
  nlayers: 3
  dropout: 0.2
  nhid: 512 #384 #256 #512
  ninp: 768
  ntoken: 1 #2
  #pooling_type: None #'mean'
  pooling_type: 'mean'
  freeze_layers: 10
embedding:
  pad_len: 128
  embedding_dim: 384
  num_embeddings: 1200
encoder:
  hidden_size: 384  # hidden size
  layers: 6  # layers
  heads: 12  # num heads
  dim_head: 32  # heads * dim_head: intermediate size
  ff_mult: 4  # Feed forward multiplier
  ema_decay: 0.998
  weights: "test_base_1e3_0/11/model.safetensors"
  freeze_layers: 10
  type: chemberta
predictor:
  dim_head: 96
  ff_mult: 4
  fine_tune_with_class_token: false
  freeze_layers: 10
  heads: 8
  hidden_size: 768
  layers: 4
  transform: false
  weights: "test_base_1e3_0/11/model_2.safetensors"
label_col: 'targets' #'value' #'mean' #'LogFC_CDH3'
#var_col: 'LogFCVar_CDH3'
loss_config:
  type: "bce"
  class_index: 0
  #pos_weight: 10.0 # 0.9601 #[0.9601, 0.9689, 0.9036, 0.9672, 0.8943, 0.9528, 0.9797, 0.8862, 0.9670, 0.9547, 0.8878, 0.9534]  #8.0 #2.0 #13.75
  #pos_weight: 8.0
